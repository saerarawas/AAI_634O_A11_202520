{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saerarawas/AAI_634O_A11_202520/blob/main/week3/Hands_on_Lab_Implementing_the_ETL_Process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hands-on Lab: Implementing the ETL (Extract, Transform, Load) Process**"
      ],
      "metadata": {
        "id": "MwRtqhfoEmJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective:**\n",
        "\n",
        "In this hands-on lab, students will learn how to implement the fundamental steps of the ETL process by extracting data from multiple sources, transforming the data, and loading it into a database. Students will use Python along with libraries such as Pandas for data transformation and PyMongo for loading the data into a MongoDB database.\n",
        "\n",
        "By the end of this lab, students will be able to:\n",
        "\n",
        "* Extract data from different sources (CSV and API).\n",
        "* Clean, transform, and validate the data.\n",
        "* Load the transformed data into MongoDB.\n",
        "* Automate the ETL process by building a reusable pipeline."
      ],
      "metadata": {
        "id": "_flGWYZjEygA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-requisites:**\n",
        "\n",
        "* Basic knowledge of Python.\n",
        "* MongoDB Atlas account (or a local MongoDB instance).\n",
        "* Install the required Python libraries:\n",
        "\n"
      ],
      "metadata": {
        "id": "WNqvAT4rFKjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this Lab:**\n",
        "\n",
        "You are tasked with creating an ETL pipeline for a fictitious retail company. You will extract product and sales data from different sources (a CSV file and a REST API), transform the data by cleaning and standardizing it, and load the transformed data into MongoDB for further analysis."
      ],
      "metadata": {
        "id": "Af48YJb-FwIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Extract Data**\n",
        "\n",
        "**1.1. Extract Product Data from a CSV File**\n",
        "\n",
        "Create a CSV file named ***products.csv*** with the following data:\n",
        "\n",
        "product_id,product_name,category,price\n",
        "\n",
        "1001,Laptop,Electronics,1200\n",
        "\n",
        "1002,Smartphone,Electronics,800\n",
        "\n",
        "1003,Chair,Furniture,150"
      ],
      "metadata": {
        "id": "HdEXdoqcF4wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the data\n",
        "data = {\n",
        "    'product_id': [1001, 1002, 1003],\n",
        "    'product_name': ['Laptop', 'Smartphone', 'Chair'],\n",
        "    'category': ['Electronics', 'Electronics', 'Furniture'],\n",
        "    'price': [1200, 800, 150]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('products.csv', index=False)\n",
        "\n",
        "print('products.csv file has been created.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrqlBTTgPokg",
        "outputId": "c1b5720b-c3f3-4e3d-cd63-605e7ee32134"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "products.csv file has been created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Python and Pandas to extract the product data from this CSV file."
      ],
      "metadata": {
        "id": "3C7Wvw-WGM9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Extract data from the CSV file\n",
        "products_df = pd.read_csv('products.csv')\n",
        "print(\"Extracted Product Data:\")\n",
        "print(products_df)\n"
      ],
      "metadata": {
        "id": "1e-U7ChnGOSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c509c448-91aa-4d1f-e0d2-7ab654ac5fff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Product Data:\n",
            "   product_id product_name     category  price\n",
            "0        1001       Laptop  Electronics   1200\n",
            "1        1002   Smartphone  Electronics    800\n",
            "2        1003        Chair    Furniture    150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2. Extract Sales Data from a REST API**\n",
        "\n",
        "For the sales data, we will simulate an API response using a dictionary. In a real-world scenario, you would use the requests library to fetch data from an API."
      ],
      "metadata": {
        "id": "ftylwwhwGRT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Simulated API response (in a real scenario, use requests.get(URL).json())\n",
        "sales_data = [\n",
        "    {\"sale_id\": \"S001\", \"product_id\": \"1001\", \"quantity\": 2, \"total\": 2400},\n",
        "    {\"sale_id\": \"S002\", \"product_id\": \"1002\", \"quantity\": 1, \"total\": 800},\n",
        "    {\"sale_id\": \"S003\", \"product_id\": \"1003\", \"quantity\": 4, \"total\": 600}\n",
        "]\n",
        "\n",
        "print(\"Extracted Sales Data:\")\n",
        "print(sales_data)\n"
      ],
      "metadata": {
        "id": "mcwhIOGxGXok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a33dd481-4ff6-4cbf-94b5-92d3fcc0d3df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Sales Data:\n",
            "[{'sale_id': 'S001', 'product_id': '1001', 'quantity': 2, 'total': 2400}, {'sale_id': 'S002', 'product_id': '1002', 'quantity': 1, 'total': 800}, {'sale_id': 'S003', 'product_id': '1003', 'quantity': 4, 'total': 600}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Transform Data**\n",
        "\n",
        "**2.1. Clean and Standardize the Product Data**\n",
        "\n",
        "Use Pandas to clean and transform the product data. For this example, let's assume you need to ensure the price field is numeric and filter out products that are too expensive."
      ],
      "metadata": {
        "id": "QARgvzg4Gaem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv('products.csv')\n",
        "\n",
        "# Ensure the price field is numeric\n",
        "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
        "\n",
        "# Filter out products that are too expensive (price > 1000)\n",
        "df_filtered = df[df['price'] <= 1000]\n",
        "\n",
        "# Display the cleaned and filtered DataFrame\n",
        "print(df_filtered)\n",
        "\n",
        "# Save the cleaned and filtered DataFrame to a new CSV file\n",
        "df_filtered.to_csv('cleaned_products.csv', index=False)\n",
        "\n",
        "print('cleaned_products.csv file has been created.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPQrKmOTGf8M",
        "outputId": "6f2be827-4537-4886-b3ea-42ebeeb2a65a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   product_id product_name     category  price\n",
            "1        1002   Smartphone  Electronics    800\n",
            "2        1003        Chair    Furniture    150\n",
            "cleaned_products.csv file has been created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2. Enrich the Sales Data**\n",
        "\n",
        "For the sales data, we'll perform a simple enrichment by adding the product_name to each sale by joining the sales_data and products_df on the product_id."
      ],
      "metadata": {
        "id": "XWTlr7q-GiMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sales_data to a DataFrame\n",
        "sales_df = pd.DataFrame(sales_data)\n",
        "\n",
        "# Convert 'product_id' to numeric in sales_df before merging\n",
        "sales_df['product_id'] = pd.to_numeric(sales_df['product_id'])\n",
        "\n",
        "# Join sales data with product data to add product_name\n",
        "sales_df = pd.merge(sales_df, products_df[['product_id', 'product_name']], on='product_id', how='left')\n",
        "print(\"Enriched Sales Data:\")\n",
        "print(sales_df)\n"
      ],
      "metadata": {
        "id": "-AE458wsGniG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e50cd393-03d7-4b56-811b-5224986a2ff8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enriched Sales Data:\n",
            "  sale_id  product_id  quantity  total product_name\n",
            "0    S001        1001         2   2400       Laptop\n",
            "1    S002        1002         1    800   Smartphone\n",
            "2    S003        1003         4    600        Chair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Load Data into MongoDB**\n",
        "\n",
        "Now that the data is transformed and cleaned, load the product and sales data into MongoDB.\n",
        "\n",
        "**3.1. Connect to MongoDB**\n",
        "\n",
        "Ensure you have MongoDB running locally or use MongoDB Atlas. Connect to MongoDB using PyMongo."
      ],
      "metadata": {
        "id": "N_J9biezGzQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo\n",
        "!pip install --upgrade pymongo\n",
        "\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "uri = \"mongodb+srv://tsjannoun123:KufyyNNqnno0atX9@cluster0.sb8py.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "# Create a new client and connect to the server\n",
        "client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "# Send a ping to confirm a successful connection\n",
        "try:\n",
        "    client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# Access a specific database\n",
        "db = client['retail_db']\n",
        "\n",
        "# Access a collection within the database\n",
        "#collection = db['sales']\n",
        "\n"
      ],
      "metadata": {
        "id": "J-WpayvQG3NP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6252d0ca-79c6-41c6-870f-f57b6438f1ae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo\n",
            "  Downloading pymongo-4.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading pymongo-4.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.7.0 pymongo-4.11\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.11/dist-packages (4.11)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo) (2.7.0)\n",
            "Pinged your deployment. You successfully connected to MongoDB!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2. Load Product Data**\n",
        "\n",
        "Insert the transformed product data into the MongoDB products collection."
      ],
      "metadata": {
        "id": "KdbvO_ynG5W3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DataFrame to dictionary and insert into MongoDB\n",
        "#product_records = filtered_products_df.to_dict(orient='records')\n",
        "#db.products.insert_many(product_records)\n",
        "#print(\"Loaded Product Data into MongoDB\")\n",
        "# Convert DataFrame to dictionary and insert into MongoDB\n",
        "# The DataFrame df_filtered created in the transform step (ipython-input-7-699028617ea2) is used here.\n",
        "product_records = df_filtered.to_dict(orient='records')\n",
        "db.products.insert_many(product_records)\n",
        "print(\"Loaded Product Data into MongoDB\")"
      ],
      "metadata": {
        "id": "yFPviZ0jG-hM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02e5de1a-773e-4395-fe27-f9d89eb7eca3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Product Data into MongoDB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3. Load Sales Data**\n",
        "\n",
        "Insert the enriched sales data into the MongoDB sales collection."
      ],
      "metadata": {
        "id": "RcPF_GfEHBcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DataFrame to dictionary and insert into MongoDB\n",
        "sales_records = sales_df.to_dict(orient='records')\n",
        "db.sales.insert_many(sales_records)\n",
        "print(\"Loaded Sales Data into MongoDB\")\n"
      ],
      "metadata": {
        "id": "xGqJWM2sHHSh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5549170d-5766-4c83-ecc9-da5d643aac9f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Sales Data into MongoDB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Automate the ETL Process**\n",
        "\n",
        "To make the ETL process reusable, wrap the steps into functions and run the ETL pipeline from start to finish."
      ],
      "metadata": {
        "id": "0w-Iu3KuHKGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_products():\n",
        "    return pd.read_csv('products.csv')\n",
        "\n",
        "def extract_sales():\n",
        "    return pd.DataFrame(sales_data)\n",
        "\n",
        "def transform_products(products_df):\n",
        "    products_df['price'] = pd.to_numeric(products_df['price'], errors='coerce')\n",
        "    return products_df[products_df['price'] < 1000]\n",
        "\n",
        "def transform_sales(sales_df, products_df):\n",
        "    # Convert 'product_id' to numeric in sales_df before merging\n",
        "    sales_df['product_id'] = pd.to_numeric(sales_df['product_id'], errors='coerce')\n",
        "    # Now you can merge\n",
        "    return pd.merge(sales_df, products_df[['product_id', 'product_name']], on='product_id', how='left')\n",
        "\n",
        "def load_data(products_df, sales_df):\n",
        "    db.products.insert_many(products_df.to_dict(orient='records'))\n",
        "    db.sales.insert_many(sales_df.to_dict(orient='records'))\n",
        "\n",
        "# Run the ETL pipeline\n",
        "products_df = extract_products()\n",
        "sales_df = extract_sales()\n",
        "transformed_products_df = transform_products(products_df)\n",
        "transformed_sales_df = transform_sales(sales_df, products_df)\n",
        "load_data(transformed_products_df, transformed_sales_df)\n",
        "print(\"ETL Process Completed!\")"
      ],
      "metadata": {
        "id": "p-h6IeqiHRIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f1ef10f-ed09-41c8-c94d-914c830ec3a7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETL Process Completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "This hands-on lab provides a comprehensive introduction to the ETL process, from extracting raw data from multiple sources, transforming it for quality and consistency, and finally loading it into MongoDB."
      ],
      "metadata": {
        "id": "IJ2xNTfeEjAk"
      }
    }
  ]
}